import numpy as np
import copy
from sklearn.cluster import SpectralClustering
import globalvar as gl
from fp_growth_plus import load_data,Fp_growth_plus
from utils import list_in_list, list_solved_list,read_query_data,prune_affinity_matrix
# from DiskCost import cal_total_cost_update
from DiskCost import cal_total_cost_improvement as cal_total_cost_update
import DiskCost as DC
import ray
import threading
import multiprocessing as mul
from functools import  partial
lock = threading.Lock()
L_GLOBAL = []
QUERYS=[]
ATTRS_LENGTH=[]
import warnings
def get_freq_set_by_range(complete_column):
    global L_GLOBAL
    part_L = []
    for item in reversed(L_GLOBAL):
        for key in item:
            if list_in_list([x for x in key], complete_column):
                part_L.append({key: item[key]})
    return part_L


def normalizePartition(candidate_paritions, accessed_attributes):
    new_candidate_paritions = []
    accessed_attributes = accessed_attributes.tolist()
    unaccessed_un_par = [[]]
    unaccessed_complete_par = []
    for i in range(len(ATTRS_LENGTH)):
        if (i not in accessed_attributes):
            unaccessed_un_par[0].append(i)
            unaccessed_complete_par.append([i])
    for par in candidate_paritions:
        new_par = []
        for attr in par:
            if attr in accessed_attributes: new_par.append(attr)
        if (len(new_par) > 0): new_candidate_paritions.append(new_par)
    cost1 = cal_total_cost_update(QUERYS, new_candidate_paritions + unaccessed_un_par, ATTRS_LENGTH)
    cost2 = cal_total_cost_update(QUERYS, new_candidate_paritions + unaccessed_complete_par, ATTRS_LENGTH)
    if cost1 < cost2:
        return new_candidate_paritions + unaccessed_un_par
    else:
        return new_candidate_paritions + unaccessed_complete_par

def compute_cost_by_spectal_cluster(file_input_info):
    warnings.filterwarnings('ignore')
    global L_GLOBAL
    global QUERYS
    global ATTRS_LENGTH
    affinity_matrix, QUERYS = read_query_data(file_input_info['path'], file_input_info['attr_num'])
    ATTRS_LENGTH = file_input_info['attrs_length']
    if 'tab' in file_input_info:
        DC.tab_name=file_input_info['tab']
    X, accessedAttr = prune_affinity_matrix(affinity_matrix)
    min_cost = float('inf')
    min_cluster = []
    # for index, k in enumerate(range(2,math.ceil(attr_num/2)+1)):
    # for index, k in enumerate(range(2,math.ceil(file_input_info['attr_num']/2)+1)):
    # with warnings.catch_warnings():
    #     warnings.filterwarnings(
    #         "ignore",
    #         message="Graph is not fully connected, spectral embedding may not work as expected.", category=UserWarning)
    for k in range(1, len(X) + 1):
        y_pred = SpectralClustering(n_clusters=k, affinity='precomputed',
                                    assign_labels="discretize",
                                    random_state=0).fit_predict(X)
        # print(y_pred)
        candidate_paritions = []
        for i in range(k):
            class_label = np.where(y_pred == i)[0]
            candidate_paritions.append(accessedAttr[class_label.tolist()].tolist())
        candidate_paritions_normalization = normalizePartition(candidate_paritions, accessedAttr)
        cost = cal_total_cost_update(QUERYS, candidate_paritions_normalization, ATTRS_LENGTH)
        # cost=cal_total_memory_cost(querys,candidate_paritions,attrs_length)
        if cost < min_cost:
            min_cost = cost
            min_cluster = candidate_paritions_normalization
        # print ("Calinski-Harabasz-me Score ï¼šn_clusters=", k,"score:", calinski_harabasz_score_me(X, y_pred,k))
    # while([] in min_cluster):min_cluster.remove([])
    # print("åŸå§‹åˆ†åŒº:", min_cluster)
    data_set, _ = load_data(accessedAttr,QUERYS)
    min_support = 0
    L_GLOBAL, _ = Fp_growth_plus().generate_L(data_set, min_support)

    optimal_candidate_paritions = get_best_partition_res(min_cluster)
    # optimal_candidate_paritions=normalizePartition(optimal_candidate_paritions,accessedAttr,querys,attrs_length)
    # cost=cal_total_cost_update(querys,optimal_candidate_paritions,attrs_length)
    # min_cost=cal_total_memory_cost(querys,candidate_clusters,attrs_length)
    # execution_cost=cal_subtable_by_partition_time(querys,file_input_info['path'],optimal_candidate_paritions)
    # print("åˆå¹¶ååˆ†åŒº:", optimal_candidate_paritions)
    # return normalizePartition(optimal_candidate_paritions,accessedAttr,querys,attrs_length)
    return optimal_candidate_paritions

# @ray.remote
# def processNumberFreqset(itemset, complete_column_range, temp_candidates):
#     freq_item_dict = []
#     for key in itemset:
#         temp_complete_column_range = complete_column_range.copy()
#         [temp_complete_column_range.remove(x) for x in key]
#         if len(temp_complete_column_range) == 0: continue
#         reward_res = cut_reward_fun_update([[x for x in key], temp_complete_column_range],temp_candidates)
#         if (reward_res['val'] >= 0):
#             freq_item_dict.append(reward_res)
#     return freq_item_dict

# é€’å½’å‡½æ•°
def split_candidate_parition_by_cut_reward(complete_column_range, temp_candidates2):
    # å¦‚æœè¦åˆ†å‰²çš„èšç°‡åªæœ‰ä¸€ä¸ªå…ƒç´ ï¼Œç›´æ¥è¿”å›å³å¯
    if (len(complete_column_range) == 1): return [complete_column_range]
    temp_candidates = temp_candidates2.copy()
    L = get_freq_set_by_range(complete_column_range)
    freq_item_dict=[]
    # chunk_freq_set = [processNumberFreqset.remote(itemset, complete_column_range, temp_candidates)
    #                   for itemset in reversed(L)]
    for itemset in reversed(L):
        for key in itemset:
            temp_complete_column_range=complete_column_range.copy()
            [temp_complete_column_range.remove(x) for x in key]
            if len(temp_complete_column_range)==0:continue
            reward_res=cut_reward_fun_update([[x for x in key],temp_complete_column_range],temp_candidates)
            if(reward_res['val']>=0):
                freq_item_dict.append(reward_res)
    # res = ray.get(chunk_freq_set)
    # freq_item_dict = [item for list in res for item in list if len(list) != 0]
    splited_paritions = []
    left_unsplited_par = copy.deepcopy(complete_column_range)
    while (True):
        # å¦‚æœé¢‘ç¹é¡¹é›†åˆ—è¡¨ä¸ºç©ºï¼Œè¡¨æ˜å€™é€‰é¡¹å·²ç»æ’åˆ—å®Œæ¯•
        if len(freq_item_dict) == 0:
            if len(left_unsplited_par) > 0: splited_paritions.append(left_unsplited_par)
            break
        # é€‰æ‹©ç¬¬ä¸€ä¸ªå¥–åŠ±æœ€å¤§çš„é¢‘ç¹é¡¹
        freq_item_dict.sort(key=lambda x: (x["val"]), reverse=True)
        current_cut_item = freq_item_dict[0]
        freq_item_dict.remove(current_cut_item)
        left_unsplited_par = list(set(left_unsplited_par) - set(current_cut_item['fre_item'][0]))
        # è¢«åˆ‡å‰²è¿‡çš„åˆ†åŒº ä¹Ÿæœ‰å¯èƒ½ä¼šè¢«äºŒæ¬¡åˆ‡å‰²
        if (len(left_unsplited_par) == 0):
            other_temp_candidates = temp_candidates.copy()
        else:
            other_temp_candidates = [left_unsplited_par] + temp_candidates.copy()
        splited_paritions += split_candidate_parition_by_cut_reward( current_cut_item['fre_item'][0],
                                                                     other_temp_candidates)

        temp_candidates.append(current_cut_item['fre_item'][0])
        if (len(left_unsplited_par) == 0):
            break
        elif (len(left_unsplited_par) == 1):
            splited_paritions.append(left_unsplited_par)
            break
        # i=0
        for i in range(len(freq_item_dict) - 1, -1, -1):
            freq_item = freq_item_dict[i]
            if list_in_list(freq_item['fre_item'][0], left_unsplited_par):
                # main_frament_costç”±äºæ˜¯ç›¸å¯¹æ¯”è¾ƒï¼Œæ‰€ä»¥ä¸éœ€è¦å†è®¡ç®—
                n_avg_sel = 0
                update_freq_item = update_reward_fun_update(current_cut_item, freq_item,
                                                             temp_candidates)
                if len(freq_item['fre_item'][0]) == len(left_unsplited_par):
                    update_freq_item['val'] = 0
                if (update_freq_item['val'] >= 0):
                    freq_item_dict[i] = update_freq_item
                else:
                    freq_item_dict.remove(freq_item)
            else:
                freq_item_dict.remove(freq_item)

    return splited_paritions

# @ray.remote
def get_atom_partition(split_cluster,candidate_clusters,queue):
    temp_clusters = candidate_clusters.copy()
    temp_clusters.remove(split_cluster)
    init_cost = cal_total_cost_update(QUERYS,[split_cluster], ATTRS_LENGTH)
    new_split_clusters = split_candidate_parition_by_cut_reward(split_cluster,temp_clusters)
    splited_update_cost = cal_total_cost_update(QUERYS, new_split_clusters, ATTRS_LENGTH)
    # print(split_cluster,init_cost,new_split_clusters,splited_update_cost)
    if splited_update_cost < init_cost:
       queue.put({'o_clusters': split_cluster,
                'u_clusters': new_split_clusters})
    else:
        queue.put({})


def get_best_partition_res(candidate_clusters_orgin):
    candidate_clusters = candidate_clusters_orgin.copy()
    to_split_clusters = []
    for item in candidate_clusters_orgin:
        if len(item) > 1:
            to_split_clusters.append(item)
    queue=mul.Queue()
    jobs=[]
    for split_cluster in to_split_clusters:
        process=mul.Process(target=get_atom_partition,args=(split_cluster, candidate_clusters,queue))
        process.start()
        jobs.append(process)
    # for job in jobs:job.join()
    chunk_splited_cluster=[queue.get() for _ in jobs]
    for idx,item in enumerate(chunk_splited_cluster):
        if item:
            candidate_clusters.remove(item['o_clusters'])
            candidate_clusters+=item['u_clusters']
    # å¯»æ‰¾åˆå¹¶æ–¹æ¡ˆ
    combine_schema = combine_candidate_parition_by_combine_reward(candidate_clusters)
    for item in combine_schema:
        for par in item['o_clusters']:
            candidate_clusters.remove(par)
        candidate_clusters += item['u_clusters']
    return candidate_clusters

def get_best_partition_res2(candidate_clusters_orgin):
    candidate_clusters = candidate_clusters_orgin.copy()
    to_split_clusters = []
    split_schema = []
    for item in candidate_clusters_orgin:
        if len(item) > 1:
            to_split_clusters.append(item)
    for split_cluster in to_split_clusters:
        # init_cost=cal_total_cost_update(querys,[split_cluster],attrs_length)
        init_cost = cal_total_cost_update(QUERYS, [split_cluster], ATTRS_LENGTH)
        # init_cost=cal_total_memory_cost(querys,[split_cluster],attrs_length)
        temp_clusters = candidate_clusters.copy()
        temp_clusters.remove(split_cluster)
        new_split_clusters = split_candidate_parition_by_cut_reward( split_cluster,
                                                                    temp_clusters)
        # splited_update_cost=cal_total_cost_update(querys,new_split_clusters,attrs_length)
        splited_update_cost = cal_total_cost_update(QUERYS, new_split_clusters, ATTRS_LENGTH)
        # splited_update_cost=cal_total_memory_cost(querys,new_split_clusters,attrs_length)
        if splited_update_cost < init_cost:
            # print(split_cluster,"åˆ‡å‰²æˆ",new_split_clusters,"æˆæœ¬é™ä½ ",(init_cost-splited_update_cost))
            split_schema.append({
                'o_clusters': [split_cluster],
                'u_clusters': new_split_clusters,
                'reward': init_cost - splited_update_cost
            })
            candidate_clusters.remove(split_cluster)
            for new_cluster in new_split_clusters:
                candidate_clusters.append(new_cluster)
    # print(split_schema)
    # å¯»æ‰¾åˆå¹¶æ–¹æ¡ˆ
    combine_schema = combine_candidate_parition_by_combine_reward(candidate_clusters)
    # print(combine_schema)
    for item in combine_schema:
        for par in item['o_clusters']:
            candidate_clusters.remove(par)
        candidate_clusters += item['u_clusters']
    # å½’å¹¶æ–¹æ¡ˆ final_schema
    # for item1 in reversed(split_schema):
    #     for item2 in reversed(combine_schema):
    #         if item1['o_clusters'][0] in item2['o_clusters']:
    #             if item1['reward'] > item2['reward']:combine_schema.remove(item2)
    #             else: split_schema.remove(item1)
    # final_schema=split_schema+combine_schema
    # final_partitions=candidate_clusters_orgin.copy()
    # for item in final_schema:
    #     for par in item['o_clusters']:
    #         final_partitions.remove(par)
    #     final_partitions+=item['u_clusters']
    # print("åˆ†å‰²+åˆå¹¶ï¼š",final_partitions)
    return candidate_clusters

def rayProcessNumberFreqset(itemset, to_combined_clusters, temp_combined_cluster, init_cost):
    freq_item_dict = []
    for key in itemset:
        if len(key) < 2: continue
        # åˆ¤æ–­åŸç°‡æ–¹æ¡ˆæ˜¯å¦å¯ä»¥ç»„æˆè¯¥é¢‘ç¹é¡¹é›†
        temp_key = []
        for item_cluster in to_combined_clusters:
            if list_solved_list(item_cluster, key):
                temp_key.append(item_cluster)
        if set(key) <= set([y for x in temp_key for y in x]):
            if temp_key in temp_combined_cluster: continue
            lock.acquire()
            temp_combined_cluster.append(temp_key)
            lock.release()
            # è®¡ç®—åˆå¹¶æ”¶ç›Š
            temp_combined_clusters = to_combined_clusters.copy()
            [temp_combined_clusters.remove(cluster) for cluster in temp_key]
            temp_combined_clusters.append([x for item in temp_key for x in item])
            combined_cost = cal_total_cost_update(QUERYS, temp_combined_clusters, ATTRS_LENGTH)
            if init_cost - combined_cost > 0:
                freq_item_dict.append({'item': temp_key, 'val': init_cost - combined_cost})
    return freq_item_dict


from MyThread import MyThread


def combine_candidate_parition_by_combine_reward(candidate_clusters_orgin):
    # combine the splitted clusters
    to_combined_clusters = candidate_clusters_orgin.copy()
    init_cost = cal_total_cost_update(QUERYS, to_combined_clusters, ATTRS_LENGTH)
    L = L_GLOBAL
    freq_item_dict = []
    temp_combined_cluster = []
    res = []
    for itemset in reversed(L):
        task = MyThread(rayProcessNumberFreqset,
                        (itemset, to_combined_clusters, temp_combined_cluster, init_cost))
        task.start()
        res.append(task.get_result())

    # chunk_freq_set=[rayProcessNumberFreqset(itemset,to_combined_clusters,temp_combined_cluster,init_cost,querys,attrs_length) for itemset in reversed(L)]
    # res=ray.get(chunk_freq_set)
    freq_item_dict = [item for list in res for item in list if len(list) != 0]
    # for itemset in reversed(L):
    #     for key in itemset:
    #         if len(key) < 2: continue
    #         # åˆ¤æ–­åŸç°‡æ–¹æ¡ˆæ˜¯å¦å¯ä»¥ç»„æˆè¯¥é¢‘ç¹é¡¹é›†
    #         temp_key = []
    #         for item_cluster in to_combined_clusters:
    #             if list_solved_list(item_cluster, key):
    #                 temp_key.append(item_cluster)
    #         if set(key) <= set([y for x in temp_key for y in x]):
    #             if temp_key in temp_combined_cluster:continue
    #             temp_combined_cluster.append(temp_key)
    #             # è®¡ç®—åˆå¹¶æ”¶ç›Š
    #             temp_combined_clusters = to_combined_clusters.copy()
    #             [temp_combined_clusters.remove(cluster) for cluster in temp_key]
    #             temp_combined_clusters.append([x for item in temp_key for x in item])
    #             combined_cost = cal_total_cost_update(querys, temp_combined_clusters, attrs_length)
    #             if init_cost - combined_cost > 0:
    #                 freq_item_dict.append({'item': temp_key, 'val': init_cost - combined_cost})
    combine_schema = []
    left_uncombined_par = to_combined_clusters.copy()
    freq_item_dict.sort(key=lambda x: (x["val"]), reverse=True)
    # print(freq_item_dict[0:30])
    while (True):
        if len(freq_item_dict) == 0: break
        # é€‰æ‹©ç¬¬ä¸€ä¸ªå¥–åŠ±æœ€å¤§çš„é¢‘ç¹é¡¹
        freq_item_dict.sort(key=lambda x: (x["val"]), reverse=True)
        current_combined_item = freq_item_dict[0]['item']
        combine_schema.append({
            'o_clusters': current_combined_item,
            'u_clusters': [[x for item in current_combined_item for x in item]],
            'reward': freq_item_dict[0]['val']
        })
        del freq_item_dict[0]
        [left_uncombined_par.remove(x) for x in current_combined_item]
        if (len(left_uncombined_par) == 1):
            break
        for i in range(len(freq_item_dict) - 1, -1, -1):
            freq_item = freq_item_dict[i]
            # temp_left_uncombined_par = left_uncombined_par.copy()
            flag = True
            for par in freq_item['item']:
                if par in current_combined_item:
                    flag = False
                    freq_item_dict.remove(freq_item)
                    break
    return combine_schema


def cut_reward_fun_update(splited_column, temp_candidates2):
    temp_candidates = temp_candidates2.copy()
    # åªè®¿é—®bæ®µå±æ€§çš„æŸ¥è¯¢æ•°é‡ä¸ºğ‘›_1,åªè®¿é—®cæ®µå±æ€§çš„æŸ¥è¯¢æ•°é‡ä¸ºğ‘›_2ï¼Œæ—¢è®¿é—®båˆè®¿é—®cæ®µå±æ€§çš„æŸ¥è¯¢æ•°é‡ä¸ºğ‘›_3ã€‚
    b_parition = splited_column[0]
    c_parition = splited_column[1]
    res = {
        'fre_item': [b_parition, c_parition],
        # è€ƒè™‘æ•´ä½“åˆ†åŒºæ–¹æ¡ˆçš„ï¼Œè€Œä¸æ˜¯åŸå§‹ç°‡å†…çš„å­åˆ†åŒº
        'val': cal_total_cost_update(QUERYS, temp_candidates + [b_parition + c_parition],
                                     ATTRS_LENGTH) - cal_total_cost_update(QUERYS, splited_column + temp_candidates,
                                                                           ATTRS_LENGTH),
    }
    return res


# ç”±äºå·²ç»ç¡®å®šå¥½åˆ‡å‰²çš„é¢‘ç¹é¡¹ï¼Œæ‰€ä»¥avg_sel,n3_matrix_indå¯ä»¥äº‹å…ˆè®¡ç®—å‡ºæ¥ï¼Œä¸å½±å“update
def update_reward_fun_update(last_cut_info, my_cut_info,temp_candidates2):
    temp_candidates = temp_candidates2.copy()
    before_change_par = [last_cut_info['fre_item'][1]]
    after_change_par = [my_cut_info['fre_item'][0],
                        list(set(my_cut_info['fre_item'][1]) - set(last_cut_info['fre_item'][0]))]
    res = {
        'fre_item': [my_cut_info['fre_item'][0],
                     list(set(my_cut_info['fre_item'][1]) - set(last_cut_info['fre_item'][0]))],
        # 'val':delta_main+delta_sec,
        # 'val':cal_total_cost_update(querys,[last_cut_info['fre_item'][1]],attrs_length)-cal_total_cost_update(querys,[my_cut_info['fre_item'][0],list(set(my_cut_info['fre_item'][1])-set(last_cut_info['fre_item'][0]))],attrs_length),
        # 'val':cal_total_memory_cost(querys,[last_cut_info['fre_item'][1]],attrs_length)-cal_total_memory_cost(querys,[my_cut_info['fre_item'][0],list(set(my_cut_info['fre_item'][1])-set(last_cut_info['fre_item'][0]))],attrs_length)
        'val': cal_total_cost_update(QUERYS, temp_candidates + before_change_par, ATTRS_LENGTH) - cal_total_cost_update(
            QUERYS, temp_candidates + after_change_par, ATTRS_LENGTH)
    }
    return res

